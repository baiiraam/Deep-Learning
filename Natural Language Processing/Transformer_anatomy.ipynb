{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7tRZvLNl0M6tidQG+H35u"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "669P-0mamRkw"
      },
      "outputs": [],
      "source": [
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from bertviz.transformers_neuron_view import BertModel\n",
        "from bertviz.neuron_view import show\n",
        "\n",
        "model_ckpt = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = BertModel.from_pretrained(model_ckpt)\n",
        "text = 'time flies like an arrow'\n",
        "show(model, 'bert', tokenizer, text, display_mode = 'light', layer = 0, head = 0)"
      ],
      "metadata": {
        "id": "PmWGdONZp94_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "text = 'time flies like an arrow'"
      ],
      "metadata": {
        "id": "e8LDZQNQp97s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer(text, return_tensors = 'pt', add_special_tokens = False)\n",
        "input.input_ids"
      ],
      "metadata": {
        "id": "HVuiJnM0p9-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_ckpt)\n",
        "token_embd = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "token_embd"
      ],
      "metadata": {
        "id": "f28AEGN-p-wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_embeds =  token_embd(input.input_ids)\n",
        "inputs_embeds.size()"
      ],
      "metadata": {
        "id": "iyn0wWKap-y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from math import sqrt\n",
        "\n",
        "query = key = value = inputs_embeds\n",
        "dim_k = key.size(-1)\n",
        "scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
        "scores.size()"
      ],
      "metadata": {
        "id": "ufxaG5pwp-1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "weights = softmax(scores, dim = -1)\n",
        "weights.sum(dim = -1)"
      ],
      "metadata": {
        "id": "em1-NZnwvd_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "id": "RgemnuoxxBqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_outputs = torch.bmm(weights, value)\n",
        "attn_outputs.shape"
      ],
      "metadata": {
        "id": "4uziazlgxZ-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "  dim_k = key.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
        "  weights = softmax(scores, dim = -1)\n",
        "  return torch.bmm(weights, value)"
      ],
      "metadata": {
        "id": "dMHe6SGXxhyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embed_dim, head_dim)\n",
        "    self.k = nn.Linear(embed_dim, head_dim)\n",
        "    self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    attn_outputs = scaled_dot_product_attention(\n",
        "        self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)\n",
        "    )\n",
        "    return attn_outputs"
      ],
      "metadata": {
        "id": "gLNiD3L0yMOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    embed_dim = config.hidden_size\n",
        "    num_heads = config.num_attention_heads\n",
        "    head_dim = embed_dim // num_heads\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    x = torch.cat([h(hidden_state) for h in self.heads], dim = -1)\n",
        "    x = self.output_linear(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "QD6V9Tuh0Lh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attn = MultiHeadAttention(config)\n",
        "attn_output = multihead_attn(inputs_embeds)\n",
        "attn_output.size()"
      ],
      "metadata": {
        "id": "Fv4F7mBP3g-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import head_view\n",
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(model_ckpt, output_attentions = True)\n",
        "\n",
        "sentence_a = 'time flies like an arrow'\n",
        "sentence_b = 'fruit flies like a banana'\n",
        "\n",
        "viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors = 'pt')\n",
        "attention = model(**viz_inputs).attentions\n",
        "sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim = 1)\n",
        "tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n",
        "\n",
        "head_view(attention, tokens, sentence_b_start, heads = [8])"
      ],
      "metadata": {
        "id": "S-B7NzR44C0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Feed-Forward Layer"
      ],
      "metadata": {
        "id": "4McG0sAK5ZGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.config = config\n",
        "    self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "nNlXoSNv8yMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_forward = FeedForward(config)\n",
        "ff_outputs = feed_forward(attn_output)\n",
        "ff_outputs.size()"
      ],
      "metadata": {
        "id": "Yx6ZsG0c9fyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm2 = nn.LayerNorm(config.hidden_size)\n",
        "    self.attention = MultiHeadAttention(config)\n",
        "    self.feed_forward = FeedForward(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    hidden_state = self.layer_norm1(x)\n",
        "    x = x + self.attention(hidden_state)\n",
        "    x = x + self.feed_forward(self.layer_norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "4qx2jdMr8yaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = TransformerEncoderLayer(config)\n",
        "encoder_layer(inputs_embeds).size()"
      ],
      "metadata": {
        "id": "MoDbMkKr8yd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size,config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size,eps = 1e-12)\n",
        "        self.dropout = nn.Dropout()\n",
        "    def forward(self,input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length,dtype = torch.long).unsqueeze(0)\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        embeddings = token_embeddings+position_embeddings\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "X8B76NR58yf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embeddings(config)\n",
        "embedding_layer(input.input_ids).size()"
      ],
      "metadata": {
        "id": "eRyWEFFuIbCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(config)]\n",
        "                                    for _ in range(config.num_hidden_layers))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.embeddings(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hoPBmydUIc2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a classification head"
      ],
      "metadata": {
        "id": "8irZKCBqIeRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerForSequenceClassification(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size,config.num_labels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.encoder(x)[:,0,:]\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ibi0_eixIiJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_labels=3\n",
        "encoder_classifier=TransformerForSequenceClassification(config)\n",
        "encoder_classifier(input.input_ids).size()"
      ],
      "metadata": {
        "id": "5Itc7JHrIj4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Decoder"
      ],
      "metadata": {
        "id": "iIcNZKwVImJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len=input.input_ids.size(-1)\n",
        "mask=torch.tril(torch.ones(seq_len,seq_len)).unsqueeze(0)\n",
        "mask[0]"
      ],
      "metadata": {
        "id": "bqLAFGBnIocp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores.masked_fill(mask==0,-float('inf'))"
      ],
      "metadata": {
        "id": "SoMzcd-nIpzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query,key,value,mask=None):\n",
        "    dim_k=key.size(-1)\n",
        "    scores=torch.bmm(query,key.transpose(1,2)/sqrt(dim_k))\n",
        "    if mask is not None:\n",
        "        scores=scores.masked_fill(mask,float('-inf'))\n",
        "        weights=softmax(scores,dim=-1)\n",
        "        return torch.bmm(weights,value)"
      ],
      "metadata": {
        "id": "1eGjD1AWIrX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}