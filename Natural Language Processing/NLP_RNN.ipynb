{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/08M2xIUocSmjeN0F3iCy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "Y0o6A0o7S8uB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIwVmkLhRXU0"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
        "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
        "test_set = raw_test_set.batch(32).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for review, label in raw_train_set.take(4):\n",
        "  print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "metadata": {
        "id": "r-9PUPcLSRVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "\n",
        "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size,\n",
        "                                                   split = 'whitespace',\n",
        "                                                   standardize=\"lower_and_strip_punctuation\")\n",
        "text_vec_layer.adapt(train_set.map(lambda review, label: review))"
      ],
      "metadata": {
        "id": "1czd2NOMSw5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer.get_vocabulary()[:50]"
      ],
      "metadata": {
        "id": "KTFB7PamT34z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer.get_vocabulary()[-50:]"
      ],
      "metadata": {
        "id": "g3tqOMUbUeGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(map(str, text_vec_layer.get_vocabulary()[:50]))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ahAdJoc4VGW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer(['it was a great movie'])"
      ],
      "metadata": {
        "id": "5e1vFoVyVa_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = 64)\n",
        "embed_layer(text_vec_layer([\"it was a great movie\"]))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PBSGwRrWVp3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer"
      ],
      "metadata": {
        "id": "36bIUsT1XI5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "SMcKbaH1WOdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer = tf.keras.optimizers.Nadam(),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(train_set, epochs=2, validation_data=valid_set)\n"
      ],
      "metadata": {
        "id": "5BDPJScZXdOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights = model.layers[1].get_weights()[0]"
      ],
      "metadata": {
        "id": "X7JR63XXY_qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"embeddings.tsv\", embedding_weights, delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "Mctf6IoUhz3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = text_vec_layer.get_vocabulary()\n",
        "\n",
        "with open(\"metadata.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
        "  for word in vocab:\n",
        "    word = word if word.strip() != \"\" else \"<PAD>\"\n",
        "    f.write(f\"{word}\\n\")"
      ],
      "metadata": {
        "id": "YersI6Ech9AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Shakespearean Text Using a Character RNN"
      ],
      "metadata": {
        "id": "ZlQXUDwaiV6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "metadata": {
        "id": "KmF-gmSPk4F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_text[:80])"
      ],
      "metadata": {
        "id": "NV2OuXaYk4KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(sorted(set(shakespeare_text.lower())))"
      ],
      "metadata": {
        "id": "jdc8Xz50k4M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split = 'character',\n",
        "                                                   standardize = 'lower')\n",
        "\n",
        "text_vec_layer.adapt(shakespeare_text)\n",
        "encoded = text_vec_layer([shakespeare_text][0])"
      ],
      "metadata": {
        "id": "s98TIyEsk4PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "id": "Tp-i5SYEmpTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded -= 2\n",
        "vocab_size = text_vec_layer.vocabulary_size() - 2\n",
        "dataset_size = len(encoded)"
      ],
      "metadata": {
        "id": "6nUnjWQbm2U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "id": "7R8GIJSxnQpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size"
      ],
      "metadata": {
        "id": "dl7Ly0l_nSHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, seed = None, shuffle = False, batch_size = 32):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds = ds.window(length + 1, shift = 1, drop_remainder = True)\n",
        "  ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(100_000, seed = seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ],
      "metadata": {
        "id": "ur6zi_7ynTLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length = 100\n",
        "tf.random.set_seed(42)\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle = True,\n",
        "                       seed = 42)\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)"
      ],
      "metadata": {
        "id": "yR1k_xsvomFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='nadam',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'my_shakespeare_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    validation_data=valid_set,\n",
        "    epochs=2\n",
        ")"
      ],
      "metadata": {
        "id": "cfYs6l2yqX8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),\n",
        "    model\n",
        "])"
      ],
      "metadata": {
        "id": "CtZJZUEy1Jpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict(tf.constant(['To be or not to b']))[0, -1]\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "id": "9SeGMuw71KIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Fake Shakespearean Text"
      ],
      "metadata": {
        "id": "b9_SC4re1L3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas = tf.math.log([[0.5, 0.3, 0.2]])\n",
        "tf.random.categorical(log_probas, num_samples = 8)"
      ],
      "metadata": {
        "id": "pqd509gn1RJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature = 1):\n",
        "    text = tf.constant([text])\n",
        "    y_proba = shakespeare_model.predict(text)[0, -1:]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples = 1)[0, 0]\n",
        "    return text_vec_layer.get_vocabulary()[char_id + 2]"
      ],
      "metadata": {
        "id": "eBHjLwER1Sx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extent_text(text, chars = 50, temperature = 1):\n",
        "    for _ in range(chars):\n",
        "        text +=next_char(text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "KPsjzOoX1UR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extent_text('to be or not to b', chars = 100, temperature = 0.1)"
      ],
      "metadata": {
        "id": "kW8jUSqr1V9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An Encoder-Decoder Network for Neural Machine Translation"
      ],
      "metadata": {
        "id": "QWfMOpgZ1XiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True)\n",
        "\n",
        "# Final corrected path\n",
        "spa_txt_path = Path(path).parent / \"spa-eng_extracted\" / \"spa-eng\" / \"spa.txt\"\n",
        "\n",
        "# Read the file\n",
        "text = spa_txt_path.read_text(encoding='utf-8')\n",
        "print(text[:500])  # Print first 500 characters as a quick check"
      ],
      "metadata": {
        "id": "sJoCB-u31ajr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)"
      ],
      "metadata": {
        "id": "J0vKwmSD1cnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(sentences_en[i], \"=>\", sentences_es[i])"
      ],
      "metadata": {
        "id": "yaHlykHr1ecl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length)\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length)\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ],
      "metadata": {
        "id": "K2Esxbzd1fzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "tWBehXfB1hJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "vRKSH4hm1ic7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ],
      "metadata": {
        "id": "ifaIKO9k1j4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs = tf.keras.layers.Input(shape = [], dtype = tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape = [], dtype = tf.string)"
      ],
      "metadata": {
        "id": "P9yyySQQ1mn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "tPKDTWVI1n3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state = True)\n",
        "encoder_outputa, *encoder_state = encoder(encoder_embeddings)"
      ],
      "metadata": {
        "id": "HZqU5FHC1pYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences = True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state = encoder_state)"
      ],
      "metadata": {
        "id": "HwkKufkB1rVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation = 'softmax')\n",
        "Y_proba = output_layer(decoder_outputs)"
      ],
      "metadata": {
        "id": "VITJQzpz1s60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs],\n",
        "                       outputs = [Y_proba])\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'nadam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs = 3,\n",
        "          validation_data = ((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "HpizajfG1uS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = tf.constant([sentence_en])\n",
        "        X_dec = tf.constant(np.array(['startofseq' + translation]))\n",
        "        y_proba = model.predict((X, X_dec))[0, word_idx]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == 'endofseq':\n",
        "            break\n",
        "        translation += \" \" + predicted_word\n",
        "    return translation.strip()"
      ],
      "metadata": {
        "id": "n6STDZQu1vkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I like soccer\")"
      ],
      "metadata": {
        "id": "8V8JQyBy1w_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I like soccer and going to the beach\")"
      ],
      "metadata": {
        "id": "aKgyc8eQ1yhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional RNNs"
      ],
      "metadata": {
        "id": "F5X2wtwQ1z4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ePKIWQz2TDmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state = (True))\n",
        ")"
      ],
      "metadata": {
        "id": "ViC3HDH412oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "# encoder_state = [tf.concat(encoder_state[::2], axis = -1), #short-term ()\n",
        "#                 tf.concat(encoder_state[1::2], axis = -1)]  #long-term (1 & 3)"
      ],
      "metadata": {
        "id": "WUhtynIt136U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatenateStates(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def call(self, encoder_state):\n",
        "        return [tf.concat(encoder_state[::2], axis = -1), #short-term ()\n",
        "                tf.concat(encoder_state[1::2], axis = -1)]  #long-term (1 & 3)\n",
        "\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "concat_states = ConcatenateStates()\n",
        "encoder_state = concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "zjBBPirO15Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "07/05/2025"
      ],
      "metadata": {
        "id": "OPhH2qcR16rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "2GQYwykNQUpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('I like soccer')"
      ],
      "metadata": {
        "id": "YcZZ6htzS2Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('I like cats and dogs')"
      ],
      "metadata": {
        "id": "zX8tom2YUCUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Search"
      ],
      "metadata": {
        "id": "GpWR2ONJUn85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = tf.constant([sentence_en])  # encoder input\n",
        "    X_dec = tf.constant([\"startofseq\"])  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = tf.constant([sentence_en])  # encoder input\n",
        "            X_dec = tf.constant([\"startofseq \" + translation])  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ],
      "metadata": {
        "id": "ZgUEYjIYVNBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_en = 'I like cats and dogs'\n",
        "translate(sentence_en)"
      ],
      "metadata": {
        "id": "DnPmNJ9uVinq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(sentence_en, beam_width=3, verbose=True)"
      ],
      "metadata": {
        "id": "dpkkzrEEVxo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanisms"
      ],
      "metadata": {
        "id": "9HQwoB9wV2e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n"
      ],
      "metadata": {
        "id": "DERQh0khWZep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim=embed_size)\n",
        "\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim=embed_size)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "C6IC0bDqWk42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
        ")\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "v778V3QuXgbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state = encoder_state)"
      ],
      "metadata": {
        "id": "VuT-uKpuX8sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "Y_proba = output_layer(attention_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer = 'nadam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3, validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "QCsUEpNXY3z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('I love cats and dogs')"
      ],
      "metadata": {
        "id": "rELmF-xLalhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('I love soccer and also going to the beach')"
      ],
      "metadata": {
        "id": "kACGMoaYZd2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('I love going to the beach')"
      ],
      "metadata": {
        "id": "fQWgNKgyd2bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model"
      ],
      "metadata": {
        "id": "zOT1woM4lscU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "vocab_size = 10000\n",
        "max_length = 50\n",
        "embed_size = 128\n",
        "num_heads = 5\n",
        "ff_dim = 512\n",
        "\n",
        "\n",
        "# Input layers\n",
        "encoder_inputs = tf.keras.Input(shape=(None, ), dtype = tf.int32, name='encoder_inputs')\n",
        "decoder_inputs = tf.keras.Input(shape=(None, ), dtype = tf.int32, name='decoder_inputs')\n",
        "\n",
        "\n",
        "# Embedding layer\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "\n",
        "# Positional Embedding\n",
        "pos_embedding_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "positions_encoder = tf.keras.layers.Lambda(lambda x: tf.range(start=0, limit = tf.shape(x)[1], delta=1))(encoder_inputs)\n",
        "positions_decoder = tf.keras.layers.Lambda(lambda x: tf.range(start=0, limit = tf.shape(x)[1], delta=1))(decoder_inputs)\n",
        "pos_embed_enc = pos_embedding_layer(positions_encoder)\n",
        "pos_embed_dec = pos_embedding_layer(positions_decoder)\n",
        "\n",
        "\n",
        "\n",
        "# Adding positions and token embeddings\n",
        "encoder_embed = encoder_embeddings + pos_embed_enc\n",
        "decoder_embed = decoder_embeddings + pos_embed_dec\n",
        "\n",
        "\n",
        "\n",
        "# Encoder self-attention\n",
        "encoder_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(encoder_embed, encoder_embed)\n",
        "encoder_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_embed + encoder_attention)\n",
        "\n",
        "\n",
        "# Encoder feed-forward\n",
        "encoder_ff = tf.keras.layers.Dense(ff_dim, activation='relu')(encoder_attention)\n",
        "encoder_ff = tf.keras.layers.Dense(embed_size)(encoder_ff)\n",
        "encoder_outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_attention + encoder_ff)\n",
        "\n",
        "\n",
        "# Decoder self-attention\n",
        "causal_mask = tf.keras.layers.Lambda(\n",
        "    lambda x: tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])), -1, 0)\n",
        ")(decoder_inputs)\n",
        "\n",
        "decoder_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, \\\n",
        "                                                       key_dim=embed_size)(decoder_embed, decoder_embed, attention_mask=causal_mask)\n",
        "decoder_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_embed + decoder_attention)\n",
        "\n",
        "\n",
        "\n",
        "# Encoder Decoder cross-attention\n",
        "cross_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(decoder_attention, encoder_outputs, encoder_outputs)\n",
        "decoder_cross = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_attention + cross_attention)\n",
        "\n",
        "\n",
        "# Decoder feed-forward\n",
        "decoder_ff = tf.keras.layers.Dense(ff_dim, activation='relu')(decoder_cross)\n",
        "decoder_ff = tf.keras.layers.Dense(embed_size)(decoder_ff)\n",
        "decoder_outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_cross + decoder_ff)\n",
        "\n",
        "\n",
        "# Final output-layer\n",
        "output_logits = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder_outputs)\n",
        "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], output_logits)"
      ],
      "metadata": {
        "id": "MHWh7zCOePTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='nadam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "9S6XrqAFnY3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "max_length = 50\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length,\n",
        "    pad_to_max_tokens = True\n",
        ")\n",
        "\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length,\n",
        "    pad_to_max_tokens = True\n",
        ")\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
      ],
      "metadata": {
        "id": "Kkdbzv1oxTtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_en(X_train).numpy(), padding='post', maxlen=max_length\n",
        ")\n",
        "X_train_dec_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_es(X_train_dec).numpy(), padding='post', maxlen=max_length\n",
        ")\n",
        "\n",
        "X_valid_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_en(X_valid).numpy(), padding='post', maxlen=max_length\n",
        ")\n",
        "X_valid_dec_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_en(X_valid_dec).numpy(), padding='post', maxlen=max_length\n",
        ")\n",
        "\n",
        "\n",
        "X_train_padded = tf.constant(X_train_padded)\n",
        "X_train_dec_padded = tf.constant(X_train_dec_padded)\n",
        "X_valid_padded = tf.constant(X_valid_padded)\n",
        "X_valid_dec_padded = tf.constant(X_valid_dec_padded)\n",
        "\n",
        "\n",
        "\n",
        "transformer.fit(\n",
        "    (X_train_padded, X_train_dec_padded),\n",
        "    Y_train,\n",
        "    epochs=3,\n",
        "    validation_data=((X_valid_padded, X_valid_dec_padded), Y_valid)\n",
        ")\n"
      ],
      "metadata": {
        "id": "d-BCvcil0QNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def translate(sentence_en):\n",
        "    # Tokenize and pad encoder input\n",
        "    X = text_vec_layer_en(tf.constant([sentence_en]))\n",
        "    X = tf.keras.preprocessing.sequence.pad_sequences(X.numpy(), padding=\"post\", maxlen=max_length)\n",
        "\n",
        "    # Start token\n",
        "    start_token = text_vec_layer_es([ 'startofseq'])[0][0]\n",
        "    end_token = text_vec_layer_es(['endofseq'])[0][0]\n",
        "\n",
        "    # Decoder input initialized with just the start token\n",
        "    decoder_input = [start_token]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        decoder_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [decoder_input], maxlen=max_length, padding=\"post\"\n",
        "        )\n",
        "\n",
        "        y_proba = transformer.predict((X, decoder_input_padded), verbose=0)[0, len(decoder_input)-1]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "\n",
        "        if predicted_word_id == end_token:\n",
        "            break\n",
        "\n",
        "        decoder_input.append(predicted_word_id)\n",
        "\n",
        "    # Map tokens back to words\n",
        "    vocab = text_vec_layer_es.get_vocabulary()\n",
        "    translated_words = [vocab[token] for token in decoder_input[1:]]  # skip start token\n",
        "\n",
        "    return ' '.join(translated_words)"
      ],
      "metadata": {
        "id": "4n9GY1L01ynG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate('I like cats and dogs'))"
      ],
      "metadata": {
        "id": "U9SrUqtb3nQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate('Yesterday I had a steak for lunch'))"
      ],
      "metadata": {
        "id": "wVLaeWeS3rjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate('Tomorrow I will go to the World Cup Finals'))"
      ],
      "metadata": {
        "id": "FZpREGVs3wsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NIm52zwK3_Th"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}