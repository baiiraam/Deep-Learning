{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multilingual Named Entity Recognition"
      ],
      "metadata": {
        "id": "bCOBsVvKHDed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "id": "OvkcieWaI12E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "load_dataset(\"xtreme\",name=\"PAN-X.de\")"
      ],
      "metadata": {
        "id": "WzzbwiGOI3Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from datasets import DatasetDict\n",
        "\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "\n",
        "for lang, frac in zip(langs, fracs):\n",
        "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "    for split in ds:\n",
        "        panx_ch[lang][split] = (\n",
        "            ds[split]\n",
        "            .shuffle(seed=0)\n",
        "            .select(range(int(frac * ds[split].num_rows))))"
      ],
      "metadata": {
        "id": "gswsyR6jI6MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags=panx_ch['de']['train'].features['ner_tags'].feature\n",
        "tags"
      ],
      "metadata": {
        "id": "_rp-iQBKI8nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tag_names(batch):\n",
        "    return {'ner_tags_str':[tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
        "\n",
        "panx_de=panx_ch['de'].map(create_tag_names)"
      ],
      "metadata": {
        "id": "c_b0cmIzI-z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilingual Transformers"
      ],
      "metadata": {
        "id": "74HQCAS-JAc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "bert_model_name='bert-base-cased'\n",
        "xlmr_model_name='xlm-roberta-base'\n",
        "bert_tokenizer=AutoTokenizer.from_pretrained(bert_model_name)\n",
        "xlmr_tokenizer=AutoTokenizer.from_pretrained(xlmr_model_name)"
      ],
      "metadata": {
        "id": "w2ZigpkM5GBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='Jack Sparrow loves Tokyo!'\n",
        "bert_tokens=bert_tokenizer(text).tokens()\n",
        "xlmr_tokens=xlmr_tokenizer(text).tokens()"
      ],
      "metadata": {
        "id": "Wd83c3ga5HXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame([bert_tokens,xlmr_tokens],\n",
        "                index=['Bert','XLM-R'])\n",
        "df"
      ],
      "metadata": {
        "id": "UZopofHU5Ja2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece Tokenizer"
      ],
      "metadata": {
        "id": "W5J0cefU5LMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(xlmr_tokens).replace('\\u2581',\" \")"
      ],
      "metadata": {
        "id": "3uJFSVTl5SNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Custom Model for Token Classification"
      ],
      "metadata": {
        "id": "qACG54Tj5TmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
        "\n",
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    config_class=XLMRobertaConfig\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels=config.num_labels\n",
        "        self.roberta=RobertaModel(config,add_pooling_layer=False)\n",
        "        self.dropout=nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier=nn.Linear(config.hidden_size,config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self,input_ids,attention_mask=None,token_type_ids=None,\n",
        "                labels=None,**kwargs):\n",
        "        outputs=self.roberta(input_ids,attention_mask=attention_mask,\n",
        "                             token_type_ids=token_type_ids,**kwargs)\n",
        "        sequence_output=self.dropout(outputs[0])\n",
        "        logits=self.classifier(sequence_output)\n",
        "        loss=None\n",
        "        if labels is not None:\n",
        "            loss_fct=nn.CrossEntropyLoss()\n",
        "            loss=loss_fct(logits.view(-1,self.num_labels,labels.view(-1)))\n",
        "        return TokenClassifierOutput(loss=loss,logits=logits,\n",
        "                                     hidden_states=outputs.hidden_states,\n",
        "                                     attentions=outputs.attentions)"
      ],
      "metadata": {
        "id": "9eL3WUQP5XcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a Custom Model"
      ],
      "metadata": {
        "id": "AwsyDoTg5Z5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag={idx: tag for idx,tag in enumerate(tags.names)}\n",
        "tag2index={tag: idx for idx,tag in enumerate(tags.names)}"
      ],
      "metadata": {
        "id": "BRoPMxvT5cqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags.names"
      ],
      "metadata": {
        "id": "q0Lry3su5d4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "xlmr_config=AutoConfig.from_pretrained(xlmr_model_name,\n",
        "                                       num_labels=tags.num_classes,\n",
        "                                       id2label=index2tag,label2id=tag2index)"
      ],
      "metadata": {
        "id": "09H5xjnc5fIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "xlmr_model=XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name,\n",
        "                                                            config=xlmr_config).to(device)"
      ],
      "metadata": {
        "id": "RuSjEGyQ5gma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids=xlmr_tokenizer.encode(text,return_tensors='pt')\n",
        "pd.DataFrame([xlmr_tokens,input_ids[0].numpy()], index=['Tokens','Input IDs'])"
      ],
      "metadata": {
        "id": "hYukws5J5iX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=xlmr_model(input_ids.to(device)).logits\n",
        "predictions=torch.argmax(outputs,dim=-1)\n",
        "print(f'Number of tokens in sequence: {len(predictions[0])}')\n",
        "print(f'Shape of outputs tensor: {outputs.shape}')"
      ],
      "metadata": {
        "id": "D8eMm0wz5jss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=[tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "pd.DataFrame([xlmr_tokens,preds],index=['Tokens','Tags'])"
      ],
      "metadata": {
        "id": "BmLN4nll5lBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntogsncj5mU3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}